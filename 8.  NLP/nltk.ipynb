{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5687249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43d2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae83691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n",
    "#Install corpora using nltk.download()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212962a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"popular\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbd475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcf7bd49",
   "metadata": {},
   "source": [
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6115240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f28b4da",
   "metadata": {},
   "source": [
    "## Natural Language Processing using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c88d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203d839",
   "metadata": {},
   "source": [
    "# data collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ad43b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04188052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.collections.LazySubsequence'> 100\n",
      "[['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.'], ...]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories='editorial')[:100]\n",
    "print(type(data), len(data))\n",
    "print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823e9f0",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "- Data Collection \n",
    "- Tokenization, Stopwards Removal, Stemming\n",
    "- Building a common vocab \n",
    "- Vectorize the documents \n",
    "- Performing Classification/Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e4700",
   "metadata": {},
   "source": [
    "## 2. Tokenization and Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "011bd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It was a very pleasant day, the weather was cool and there were showers. I went to market to buy some fruits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291506a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ad966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff51fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day, the weather was cool and there were showers.', 'I went to market to buy some fruits.']\n"
     ]
    }
   ],
   "source": [
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d46ed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'very', 'pleasant', 'day', ',', 'the', 'weather', 'was', 'cool', 'and', 'there', 'were', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "word_list = word_tokenize(sents[0].lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805bd8b",
   "metadata": {},
   "source": [
    "## Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "846341df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7efbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "271864d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wasn', 'how', 'this', 'between', 'hadn', 'to', 'has', 'because', 'why', 'd', 'our', 'doing', \"don't\", 'while', 'up', \"mightn't\", 'again', 'there', 'where', \"it's\", 'it', 'i', 'so', 'being', 'here', 'doesn', 'but', \"won't\", \"you'll\", 'my', 'have', \"couldn't\", \"she's\", \"that'll\", 'after', 'very', 'them', 'its', 'nor', \"you're\", 'o', 'few', 'hers', 'about', 'won', 'too', \"you'd\", 'needn', 'y', 'by', 'the', 'above', \"shan't\", 'these', 'those', 'shouldn', 'just', 'haven', 'weren', 'more', 'that', 'when', 'couldn', \"you've\", 'any', 'mightn', 'into', 'from', 'what', 'we', 'themselves', 'hasn', 'her', 'an', 'was', 'all', \"wasn't\", 'ourselves', 'herself', 'no', 'me', 'a', 'ain', 'once', 'own', 'are', \"hasn't\", 'did', 'under', \"doesn't\", \"wouldn't\", 'theirs', 'than', 'further', 'myself', 'as', 'will', 'yourself', 'his', 'don', 'only', \"mustn't\", \"weren't\", 'be', 'you', 'against', 'isn', 'for', 'shan', 'each', 'now', 'been', 'if', 'on', 'am', 'or', 'during', 'such', 'in', 'does', 'is', 'over', \"isn't\", 'until', 'at', \"needn't\", 'wouldn', 'should', 'he', 'aren', 'can', 'not', 'out', 'him', 'your', 'll', \"haven't\", 'ma', 've', 'didn', 'were', 'with', 'himself', \"should've\", 'itself', 'had', 'having', 'whom', 'some', 'their', 'and', 'yours', 'mustn', 're', 'do', 'off', 'both', 's', 'm', 'same', 'they', 'of', 'below', 'down', 'through', 'which', \"didn't\", 'ours', \"shouldn't\", 'yourselves', 'other', 'who', 't', 'then', \"aren't\", 'before', 'she', 'most', \"hadn't\"}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(sw)\n",
    "print(len(sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f21e9c",
   "metadata": {},
   "source": [
    "## Filter the words from the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "971e33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(word_list):\n",
    "    \n",
    "    useful_words = [w for w in word_list if w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee62e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasant', 'day', ',', 'weather', 'cool', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "useful_words = filter_words(word_list)\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35269b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer #regular expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc36211",
   "metadata": {},
   "source": [
    "Regular-Expression Tokenizers\n",
    "\n",
    "A RegexpTokenizer splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80fb6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z0-9]+\")  # word with a to z or A to z or 0-9 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfb3a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "capword_tokenizer = RegexpTokenizer(r'[A-Z]\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db21cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'the', '50', 'Documents', 'to', 'abc', 'Def', 'ghi']\n",
      "['Send', 'Documents', 'Def']\n"
     ]
    }
   ],
   "source": [
    "sents = \"Send the 50 Documents to abc, Def, ghi.\"\n",
    "print(tokenizer.tokenize(sents))\n",
    "print(capword_tokenizer.tokenize(sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6071d5",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "- Process that transforms particular words into root words\n",
    "- jumping, jump, jumps, jumped => jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46bfc271",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox was seen jumping over the lazy dog from high wall. Foxes love to make jumps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "479f2168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lazy', 'dog', 'from', 'high', 'wall', 'foxes', 'love', 'to', 'make', 'jumps']\n"
     ]
    }
   ],
   "source": [
    "word_list = tokenizer.tokenize(text.lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf227c3b",
   "metadata": {},
   "source": [
    "## Types of Stemmers \n",
    "- Snowball Stemmer (Multilingual)\n",
    "- Porter Stemmer \n",
    "- Lancaster Stemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101855ba",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer :-\n",
    "is a multilingual stemmer that can handle various languages, \n",
    "example, in English, the word \"running\" is stemmed to \"run,\" while in French, \"manger\" (to eat) is stemmed to \"mang\" (stemmed form).\n",
    "\n",
    "while the Porter and Lancaster Stemmers are English-specific stemmers.\n",
    "\n",
    "#### The Porter Stemmer :-\n",
    "applies a set of rules to obtain stems, \n",
    "the word \"running\" is stemmed to \"run,\" \"dogs\" is stemmed to \"dog,\" \n",
    "\n",
    "and \n",
    "\n",
    "#### the Lancaster Stemmer:-\n",
    "is more aggressive and applies more extensive rules\n",
    "the word \"running\" is stemmed to \"run,\" \"dogs\" is stemmed to \"dog,\" and \"jumps\" is stemmed to \"jump.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5736a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b08d6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6be3089b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"jumped\")\n",
    "ps.stem(\"jumping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "514d4724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"lovely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e1ecf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesom\n",
      "awesom\n",
      "teen\n",
      "teenag\n"
     ]
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "\n",
    "print(ps.stem(\"awesome\"))\n",
    "print(ls.stem(\"awesome\"))\n",
    "\n",
    "print(ls.stem(\"teenager\"))\n",
    "print(ps.stem(\"teenager\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86825164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cour'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SnowballStemmer('french')\n",
    "ss.stem('courais')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6268fc",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77d030be",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Indian cricket team will win world cup, says caption virat kohli, World cup will be held at India in next year.',\n",
    "    'We will win next Lok Sabha Election, says Indian PM',\n",
    "    'The nobel Rabindranath tagore won the hearts of the people', \n",
    "    'The movie Raazi is an exciting thriller based upon real story'\n",
    "]\n",
    "# corpusis the set of documents are stored i.e collection of document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36432977",
   "metadata": {},
   "source": [
    " I want to convert words into numerical features \n",
    " \n",
    " Building a common vocabulary and vectorize the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "874a85cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'raazi', 'exciting', 'thriller', 'based', 'upon', 'real', 'story']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def myTokenizer(sentence):\n",
    "    words = tokenizer.tokenize(sentence.lower())\n",
    "    return filter_words(words)\n",
    "\n",
    "list_words = myTokenizer(corpus[3])\n",
    "print(list_words)\n",
    "print(len(list_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ab9ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "290a1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer,ngram_range=(1,1)) #unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d861777",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(tokenizer=myTokenizer,ngram_range=(1,2)) #bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38b3db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(tokenizer=myTokenizer,ngram_range=(1,3)) #trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5f99a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vectorized_corpus = cv1.fit_transform(corpus)\n",
    "vectorized_corpus = cv2.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc4c48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2478fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d77df749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1\n",
      " 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(vc[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d40eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 9, 'cricket': 2, 'team': 24, 'win': 28, 'world': 29, 'cup': 3, 'says': 21, 'caption': 1, 'virat': 27, 'kohli': 10, 'held': 7, 'india': 8, 'next': 13, 'year': 30, 'lok': 11, 'sabha': 20, 'election': 4, 'pm': 16, 'nobel': 14, 'rabindranath': 18, 'tagore': 23, 'hearts': 6, 'people': 15, 'movie': 12, 'raazi': 17, 'exciting': 5, 'thriller': 25, 'based': 0, 'upon': 26, 'real': 19, 'story': 22}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2e14f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 19, 'cricket': 4, 'team': 49, 'win': 57, 'world': 60, 'cup': 6, 'says': 43, 'caption': 2, 'virat': 55, 'kohli': 22, 'held': 15, 'india': 17, 'next': 28, 'year': 62, 'indian cricket': 20, 'cricket team': 5, 'team win': 50, 'win world': 59, 'world cup': 61, 'cup says': 8, 'says caption': 44, 'caption virat': 3, 'virat kohli': 56, 'kohli world': 23, 'cup held': 7, 'held india': 16, 'india next': 18, 'next year': 30, 'lok': 24, 'sabha': 41, 'election': 9, 'pm': 34, 'win next': 58, 'next lok': 29, 'lok sabha': 25, 'sabha election': 42, 'election says': 10, 'says indian': 45, 'indian pm': 21, 'nobel': 31, 'rabindranath': 37, 'tagore': 47, 'hearts': 13, 'people': 33, 'nobel rabindranath': 32, 'rabindranath tagore': 38, 'tagore hearts': 48, 'hearts people': 14, 'movie': 26, 'raazi': 35, 'exciting': 11, 'thriller': 51, 'based': 0, 'upon': 53, 'real': 39, 'story': 46, 'movie raazi': 27, 'raazi exciting': 36, 'exciting thriller': 12, 'thriller based': 52, 'based upon': 1, 'upon real': 54, 'real story': 40}\n"
     ]
    }
   ],
   "source": [
    "print(cv1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fd3231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 28, 'cricket': 6, 'team': 70, 'win': 82, 'world': 87, 'cup': 9, 'says': 61, 'caption': 3, 'virat': 79, 'kohli': 32, 'held': 22, 'india': 25, 'next': 41, 'year': 91, 'indian cricket': 29, 'cricket team': 7, 'team win': 71, 'win world': 85, 'world cup': 88, 'cup says': 12, 'says caption': 62, 'caption virat': 4, 'virat kohli': 80, 'kohli world': 33, 'cup held': 10, 'held india': 23, 'india next': 26, 'next year': 44, 'indian cricket team': 30, 'cricket team win': 8, 'team win world': 72, 'win world cup': 86, 'world cup says': 90, 'cup says caption': 13, 'says caption virat': 63, 'caption virat kohli': 5, 'virat kohli world': 81, 'kohli world cup': 34, 'world cup held': 89, 'cup held india': 11, 'held india next': 24, 'india next year': 27, 'lok': 35, 'sabha': 58, 'election': 14, 'pm': 49, 'win next': 83, 'next lok': 42, 'lok sabha': 36, 'sabha election': 59, 'election says': 15, 'says indian': 64, 'indian pm': 31, 'win next lok': 84, 'next lok sabha': 43, 'lok sabha election': 37, 'sabha election says': 60, 'election says indian': 16, 'says indian pm': 65, 'nobel': 45, 'rabindranath': 53, 'tagore': 67, 'hearts': 20, 'people': 48, 'nobel rabindranath': 46, 'rabindranath tagore': 54, 'tagore hearts': 68, 'hearts people': 21, 'nobel rabindranath tagore': 47, 'rabindranath tagore hearts': 55, 'tagore hearts people': 69, 'movie': 38, 'raazi': 50, 'exciting': 17, 'thriller': 73, 'based': 0, 'upon': 76, 'real': 56, 'story': 66, 'movie raazi': 39, 'raazi exciting': 51, 'exciting thriller': 18, 'thriller based': 74, 'based upon': 1, 'upon real': 77, 'real story': 57, 'movie raazi exciting': 40, 'raazi exciting thriller': 52, 'exciting thriller based': 19, 'thriller based upon': 75, 'based upon real': 2, 'upon real story': 78}\n"
     ]
    }
   ],
   "source": [
    "print(cv2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f35b8ede",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 32 is out of bounds for axis 0 with size 31",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cv\u001b[39m.\u001b[39;49minverse_transform(vc[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1465\u001b[0m, in \u001b[0;36mCountVectorizer.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   1461\u001b[0m         inverse_vocabulary[X[i, :]\u001b[39m.\u001b[39mnonzero()[\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39mravel()\n\u001b[0;32m   1462\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples)\n\u001b[0;32m   1463\u001b[0m     ]\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1465\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   1466\u001b[0m         inverse_vocabulary[np\u001b[39m.\u001b[39mflatnonzero(X[i, :])]\u001b[39m.\u001b[39mravel()\n\u001b[0;32m   1467\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples)\n\u001b[0;32m   1468\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1466\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   1461\u001b[0m         inverse_vocabulary[X[i, :]\u001b[39m.\u001b[39mnonzero()[\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39mravel()\n\u001b[0;32m   1462\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples)\n\u001b[0;32m   1463\u001b[0m     ]\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m-> 1466\u001b[0m         inverse_vocabulary[np\u001b[39m.\u001b[39;49mflatnonzero(X[i, :])]\u001b[39m.\u001b[39mravel()\n\u001b[0;32m   1467\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples)\n\u001b[0;32m   1468\u001b[0m     ]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 32 is out of bounds for axis 0 with size 31"
     ]
    }
   ],
   "source": [
    "cv.inverse_transform(vc[0].reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49333da6",
   "metadata": {},
   "source": [
    "# limition of bow\n",
    "\n",
    " 1) Converting all words to the lower case. While tokenizing documents, we may encounter similar words but in different cases, eg: upper 'CASE' or lower 'case' or title 'Case'. ...\n",
    "2) Removing Stop Words. Stop words include common occurring words such as 'the', 'is', etc. ...\n",
    "3) Perform Stemming and Lemmatization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da6b50",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "\n",
    "TF - term frequency\n",
    "IDF - Inverse Document Frequency \n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a widely used technique in natural language processing (NLP) to quantify the importance of a term within a document or a corpus. In essence, it computes a weight for each term by multiplying its frequency in the document by its rarity across the corpus. This helps prioritize terms that are frequent in a document but rare in the overall corpus, thus highlighting their significance. TF-IDF is particularly useful for tasks like information retrieval, document classification, and keyword extraction in NLP.\n",
    "\n",
    "TF(w, d) = word occur in document\n",
    "IDf(w,c) = log (no of doc / no of doc with w in it )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bcca1",
   "metadata": {},
   "source": [
    "Sure! Here's a simple example to illustrate the concept of TF-IDF:\n",
    "\n",
    "Consider a corpus with two documents:\n",
    "\n",
    "Document 1: \"The cat chased the mouse.\"\n",
    "Document 2: \"The dog barked at the cat.\"\n",
    "\n",
    "To calculate the TF-IDF values for each term, we follow these steps:\n",
    "\n",
    "1. Term Frequency (TF):\n",
    "   - Count the number of times each term appears in a document.\n",
    "   - Document 1: TF(cat) = 1, TF(chased) = 1, TF(mouse) = 1.\n",
    "   - Document 2: TF(dog) = 1, TF(barked) = 1, TF(cat) = 1.\n",
    "\n",
    "2. Inverse Document Frequency (IDF):\n",
    "   - Calculate the rarity of each term across the corpus.\n",
    "   - IDF(cat) = log(2/2) = 0, as \"cat\" appears in both documents.\n",
    "   - IDF(chased) = log(2/1) = 0.301, as \"chased\" appears in only one document.\n",
    "   - IDF(mouse) = log(2/1) = 0.301, as \"mouse\" appears in only one document.\n",
    "   - IDF(dog) = log(2/1) = 0.301, as \"dog\" appears in only one document.\n",
    "   - IDF(barked) = log(2/1) = 0.301, as \"barked\" appears in only one document.\n",
    "\n",
    "3. TF-IDF Calculation:\n",
    "   - Multiply the TF and IDF values for each term in each document.\n",
    "   - Document 1: TF-IDF(cat) = 1 * 0 = 0, TF-IDF(chased) = 1 * 0.301 = 0.301, TF-IDF(mouse) = 1 * 0.301 = 0.301.\n",
    "   - Document 2: TF-IDF(dog) = 1 * 0.301 = 0.301, TF-IDF(barked) = 1 * 0.301 = 0.301, TF-IDF(cat) = 1 * 0 = 0.\n",
    "\n",
    "The resulting TF-IDF values indicate the importance of each term within each document. Higher values imply higher significance. In this example, \"chased,\" \"mouse,\" \"dog,\" and \"barked\" have the highest TF-IDF values, indicating their importance in distinguishing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdcc7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc99e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=myTokenizer, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d718248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.1678685  0.1678685  0.1678685  0.1678685\n",
      "  0.335737   0.1678685  0.1678685  0.         0.         0.\n",
      "  0.         0.         0.         0.1678685  0.1678685  0.1678685\n",
      "  0.1678685  0.13234945 0.1678685  0.         0.1678685  0.1678685\n",
      "  0.         0.         0.         0.         0.13234945 0.\n",
      "  0.1678685  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.13234945 0.1678685  0.         0.         0.\n",
      "  0.         0.1678685  0.1678685  0.         0.         0.\n",
      "  0.         0.1678685  0.1678685  0.13234945 0.         0.1678685\n",
      "  0.335737   0.335737   0.1678685 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.27230302 0.27230302 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.21468683 0.         0.27230302 0.         0.\n",
      "  0.27230302 0.27230302 0.         0.         0.21468683 0.27230302\n",
      "  0.         0.         0.         0.         0.27230302 0.\n",
      "  0.         0.         0.         0.         0.         0.27230302\n",
      "  0.27230302 0.21468683 0.         0.27230302 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.21468683 0.27230302 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.33333333 0.33333333 0.         0.\n",
      "  0.         0.33333333 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.25819889 0.25819889 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25819889\n",
      "  0.25819889 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25819889 0.25819889 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25819889\n",
      "  0.25819889 0.         0.         0.25819889 0.25819889 0.\n",
      "  0.         0.         0.         0.         0.25819889 0.\n",
      "  0.         0.         0.         0.25819889 0.25819889 0.25819889\n",
      "  0.25819889 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "{'indian': 19, 'cricket': 4, 'team': 49, 'win': 57, 'world': 60, 'cup': 6, 'says': 43, 'caption': 2, 'virat': 55, 'kohli': 22, 'held': 15, 'india': 17, 'next': 28, 'year': 62, 'indian cricket': 20, 'cricket team': 5, 'team win': 50, 'win world': 59, 'world cup': 61, 'cup says': 8, 'says caption': 44, 'caption virat': 3, 'virat kohli': 56, 'kohli world': 23, 'cup held': 7, 'held india': 16, 'india next': 18, 'next year': 30, 'lok': 24, 'sabha': 41, 'election': 9, 'pm': 34, 'win next': 58, 'next lok': 29, 'lok sabha': 25, 'sabha election': 42, 'election says': 10, 'says indian': 45, 'indian pm': 21, 'nobel': 31, 'rabindranath': 37, 'tagore': 47, 'hearts': 13, 'people': 33, 'nobel rabindranath': 32, 'rabindranath tagore': 38, 'tagore hearts': 48, 'hearts people': 14, 'movie': 26, 'raazi': 35, 'exciting': 11, 'thriller': 51, 'based': 0, 'upon': 53, 'real': 39, 'story': 46, 'movie raazi': 27, 'raazi exciting': 36, 'exciting thriller': 12, 'thriller based': 52, 'based upon': 1, 'upon real': 54, 'real story': 40}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "print(vectorized_corpus)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebeb1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56773349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a24f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac936fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4fc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
